{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cad566f",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7da3d",
   "metadata": {},
   "source": [
    "## Introduzione"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c4c24",
   "metadata": {},
   "source": [
    "Con il Topic Modeling è possibile individuare, in modo non supervisionato, i principali argomenti trattati all'interno di un corpus di documenti. Ogni **topic** è descritto come una serie di token e/o frasi rilevanti che possono essere utilizzate, successivamente, per indicizzare il documento. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a535c434",
   "metadata": {},
   "source": [
    "##### Alcuni impieghi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966da80",
   "metadata": {},
   "source": [
    "- motori di ricerca cognitivi: si taggano i contenuti con le parole chiave dei topic principali, per sfruttarle poi in fase di ricerca\n",
    "- customer service: si riesce ad individuare i topic principali discussi dai clienti e capirne più facilmente e rapidamente i bisogni\n",
    "- social network: per individuare gli argomenti principali trattati dagli utenti\n",
    "- sentiment analysis: per individuare concetti che sono espressione del sentiment del contenuto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38050b40",
   "metadata": {},
   "source": [
    "##### Le 2 tecniche principali sono:\n",
    " - Latent Semantic Analysis (LSA)\n",
    " - Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df57a0",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75b9128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf07877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb804d16",
   "metadata": {},
   "source": [
    "La libreria **spacy** sarà utilizzata solo per estrarre il lemma dalle parole operando così una diminuzione dello spazio dimensionale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d64e5d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51b4e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('it_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a1a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt', 'r') as f:\n",
    "    stopwords = f.read()\n",
    "    \n",
    "stopwords = stopwords.split(\"\\n\")\n",
    "\n",
    "# print(type(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb4bcc",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "496fa9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset_little.txt', 'r') as f:\n",
    "    documents = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acfa8c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il dataset si compone di n.72 documenti.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Il dataset si compone di n.{len(documents)} documenti.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46e313dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_documents = iter(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f978024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'è una società calcistica italiana'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_document = next(iter_documents)[64:97]\n",
    "test_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37b59df",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de09842",
   "metadata": {},
   "source": [
    "Il preprocessing si compone di alcuni semplici step:\n",
    "1. rimozione punteggiatura, numeri, ... , in sostanza restano solo le parole\n",
    "2. riduzione in minuscolo (operazione che può interferire con il NER di Spacy\n",
    "3. vengono rimosse le stopwords\n",
    "4. costruire bigrammi e trigrammi per individuare i pattern di parole più frequenti\n",
    "5. estrazione del lemma delle parole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d0d4805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estrai_solo_parole(document:str) -> str:\n",
    "    \n",
    "    document = re.sub(\"\\W|\\d\", \" \", document)\n",
    "    document = re.sub(\"[ ]+\", \" \", document)\n",
    "    document = document.lower()\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "552a653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(document:str, stopwords:list) -> str:\n",
    "    \n",
    "    list_document = document.split(\" \")\n",
    "    \n",
    "    list_document = [word for word in list_document if word not in stopwords]\n",
    "    \n",
    "    document = \" \".join(list_document)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b5e0eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(document:str) -> str:\n",
    "    \n",
    "    \"\"\"Versione che restituisce il lemma solo per i verbi\"\"\"\n",
    "    \n",
    "    doc = nlp(document)\n",
    "    \n",
    "    tokens = [token.lemma_ if token.pos_ == 'VERB' else token.text for token in doc]\n",
    "    \n",
    "    document = \" \".join(tokens)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7af9a",
   "metadata": {},
   "source": [
    "#### Raggruppamento nella funzione di preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb3f7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document:str, stopwords:list) -> str:\n",
    "    \n",
    "    \"\"\"Questa funzione raggruppa tutte le precedenti\"\"\"\n",
    "    \n",
    "    document = estrai_solo_parole(document)\n",
    "    document = remove_stopwords(document, stopwords)\n",
    "#     document = get_lemma(document)\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852bf393",
   "metadata": {},
   "source": [
    "### Costruzione del _corpus_ dei documenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e8ac7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [preprocessing(document, stopwords).split() for document in iter_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63d55caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tecnologia', 'star', 'trek', 'universo', 'star', 'trek', 'serie', 'televisiva', 'fantascientifica', 'originariamente']\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f180ba",
   "metadata": {},
   "source": [
    "### Estrazione dei pattern di parole più frequenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "836cd195",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count=5\n",
    "\n",
    "\"\"\"min_count : float, optional\n",
    "    Ignore all words and bigrams with total collected count lower than this value.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "threshold=100\n",
    "\n",
    "\"\"\"threshold : float, optional\n",
    "    Represent a score threshold for forming the phrases (higher means fewer phrases).\n",
    "\"\"\"\n",
    "\n",
    "bigram = gensim.models.Phrases(corpus, min_count=min_count, threshold=threshold) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[corpus], threshold=threshold)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eac3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_pattern(corpus:list, \n",
    "                      bigram_mod:gensim.models.phrases.FrozenPhrases, \n",
    "                      trigram_mod:gensim.models.phrases.FrozenPhrases) -> list:\n",
    "    \n",
    "    \"\"\"Va applicato a tutto il corpus documenti.\n",
    "Il corpus deve essere una lista annidata di tokens, \n",
    "quindi una lista di token per ogni lista di documenti\"\"\"\n",
    "    \n",
    "    pattern = [trigram_mod[bigram_mod[doc]] for doc in corpus]\n",
    "    \n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e682419",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = get_words_pattern(corpus, bigram_mod, trigram_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6740be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tecnologia', 'star_trek', 'universo', 'star_trek', 'serie', 'televisiva', 'fantascientifica', 'originariamente', 'ideata', 'gene']\n"
     ]
    }
   ],
   "source": [
    "print(pattern[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c3e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[(token.text, token.lemma_ ) for token in nlp(\" \".join(doc))] for doc in pattern]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3167f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the bigram and trigram models\n",
    "# print(\"\\n bigram\")\n",
    "# bigram = gensim.models.Phrases(corpus, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "# pprint(bigram.export_phrases())\n",
    "\n",
    "# print(\"\\n trigram\")\n",
    "# trigram = gensim.models.Phrases(bigram[corpus], threshold=100)\n",
    "# pprint(trigram.export_phrases())\n",
    "\n",
    "# bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# pattern = [trigram_mod[bigram_mod[doc]] for doc in corpus]\n",
    "\n",
    "# print(\"\\n patterns\")\n",
    "# print(pattern[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f64115",
   "metadata": {},
   "source": [
    "### Dizionario delle parole (Bag-of-Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3ae9c",
   "metadata": {},
   "source": [
    "Ottenute le parole più frequenti si può costruire il vocabolario di parole (**Bag-of-Words**) per l'addestramento dell'algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99533a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52ef5db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(20472 unique tokens: ['accadere', 'accurata', 'addirittura', 'alimentari', 'alphahypertreklois']...)\n"
     ]
    }
   ],
   "source": [
    "print(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c5c3d",
   "metadata": {},
   "source": [
    "## Costruzione del corpus per l'addestramento dell'algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68ad7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_doc2bow = [id2word.doc2bow(trigram_mod[bigram_mod[doc]]) for doc in pattern]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d4502c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accadere\n",
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "print(id2word[(corpus_doc2bow[0][0][0])])\n",
    "print((corpus_doc2bow[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e55a51d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = gensim.models.TfidfModel(corpus_doc2bow, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31949662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accadere\n",
      "(0, 0.03476187341468142)\n"
     ]
    }
   ],
   "source": [
    "print(id2word[(corpus_doc2bow[0][0][0])])\n",
    "print(tfidf_model[corpus_doc2bow][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0733e4",
   "metadata": {},
   "source": [
    "### Addestramento dell'algoritmo (Lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e594c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 3\n",
    "\n",
    "lda_model = gensim.models.LdaModel(tfidf_model[corpus_doc2bow], \\\n",
    "                                   num_topics=num_topics, \\\n",
    "                                   id2word=id2word, \\\n",
    "                                   passes=4, \\\n",
    "                                   random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "599e5ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model.show_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dfaf9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic n.0\n",
      "['salute', 'wimax', 'sci', 'gpp', 'sport']\n",
      "\n",
      "Topic n.1\n",
      "['podcast', 'amd', 'linguaggio', 'zeno', 'videotelefonia']\n",
      "\n",
      "Topic n.2\n",
      "['pugno', 'saluto', 'gmail', 'chopin', 'sport_motoristici']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prime 5 parole che identificano ogni Topic\n",
    "\"\"\"\n",
    "for i in range(len(lda_model.show_topics())):\n",
    "    print(f\"\\nTopic n.{i}\")\n",
    "    print([k for k,v in lda_model.show_topic(i, topn=5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3815abc6",
   "metadata": {},
   "source": [
    "## Valutazione del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2ef2c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58d11ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -11.79921384325789\n",
      "\n",
      "Coherence Score:  0.6541796373629191\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(tfidf_model[corpus_doc2bow]))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, \\\n",
    "                                     texts=[trigram_mod[bigram_mod[doc]] for doc in pattern], \\\n",
    "                                     dictionary=id2word, \\\n",
    "                                     coherence='c_v')\n",
    "\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f9b48",
   "metadata": {},
   "source": [
    "## Prediction su nuovo documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02e5d96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è una società calcistica italiana\n"
     ]
    }
   ],
   "source": [
    "new_document = test_document\n",
    "print(new_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "437a5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocessing(new_document, stopwords).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da8acdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(trigram_mod[bigram_mod[corpus]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbcfb939",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = tfidf_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c639a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model[corpus][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b30c7cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.6441914), (1, 0.19824927), (2, 0.15755938)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "812b8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model.get_document_topics(corpus)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ab8370b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.644034), (1, 0.19840541), (2, 0.15756062)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c5b89c",
   "metadata": {},
   "source": [
    "#### Estrazione Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "558b2af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_topics(topics:list, probability:float=0.0, n:int=1):\n",
    "    \n",
    "    if n <= 0:\n",
    "        n = 1\n",
    "    \n",
    "    if probability > 0:\n",
    "        topic_list = [k for k,v in sorted(topics , key=lambda item: item[1], reverse=True) if v >= probability]\n",
    "    else:\n",
    "        topic_list = [k for k,v in sorted(topics , key=lambda item: item[1], reverse=True)]\n",
    "        topic_list = topic_list[:n]\n",
    "    \n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57a60d2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_topics(topics, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae21551",
   "metadata": {},
   "source": [
    "## Salvataggio modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d78bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all trained models to disk\n",
    "lda_model.save('lda.model')\n",
    "tfidf_model.save('tfidf.model')\n",
    "id2word.save('id2word.dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83aa9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaModel.load('lda.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
